---
format: 
  revealjs:
    css: ./styles.css
    slide-number: true
    show-slide-number: all
    preview-links: auto
    embed-resources: true
    standalone: true
    progress: true
    history: true
    hash-type: number
    theme: default
    code-block-background: true
    highlight-style: zenburn
    code-link: false
    code-copy: true
    pagetitle: "Machine Learning in R with tidymodels"
    author-meta: "Jeffrey Girard"
    date-meta: "2022-10-26"
---

::: {.my-title}
# [Machine Learning]{.blue} <br />in R with tidymodels

::: {.light-silver}
[Jeffrey M. Girard | University of Kansas]{}<br />
[Technology in Psychiatry Summit 2022]{.i}
:::

![](./img/predictive_analytics_357EDD.svg){.absolute bottom=0 right=0 width=400}
:::



<!-- Overview -->

# Overview

## Instructor {.smaller}

::: {.columns .pv4}
::: {.column width="30%"}
::: {.tc}
![](./img/Girard_500x500.jpg){.br-100}

**Jeffrey Girard**, PhD<br /> [www.jmgirard.com](https://www.jmgirard.com)<br /> [jmgirard\@ku.edu](mailto:jmgirard@ku.edu)
:::
:::

::: {.column width="10%"}
:::

::: {.column width="60%"}
::: {.fragment}
#### Background

-   [Assistant Professor]{.b .green}, *University of Kansas*
-   Research Postdoc, *Carnegie Mellon University*
-   PhD Student, *University of Pittsburgh*
:::
::: {.fragment .mt1}
#### Research Areas

-   Psychological/Clinical Assessment
-   Affective/Interpersonal Communication
-   Applied Statistics and [Machine Learning]{.b .green}
-   [Data Science]{.b .green} and Software Engineering
:::
:::
:::

## Workshop Overview
1. Conceptual Introductions

2. Walkthrough #1 (Classification)

3. Walkthrough #2 (Regression)

4. Advanced Previews

5. Question and Answer

6. Practice Assignments

## Workshop Datasets

- **Walkthrough #1 (Classification)**

  [Drug Consumption](https://jmgirard.github.io/tips2022ml/data/drug_consumption.csv) $(n=1876, p_{\text{cocaine_user}}=0.36)$

  *How well can we predict who is a cocaine user based on demographics, personality traits, and legal substance use?*

::: {.fragment .mt1}
- **Walkthrough #2 (Regression)**

  [Cali Housing](https://jmgirard.github.io/tips2022ml/data/calihousing.csv) $(n=2000, \text{Range}=\$15\text{k}\text{ to }\$500\text{k})$

  *How well can we predict the median house value of a block in 1990 California based on its houses, population, and location?*
:::

<!-- Conceptual Introductions -->

# Conceptual Introductions

## What is machine learning?

- Machine learning (ML) is a **branch of computer science**

- ML researchers **develop algorithms** that [learn from data]{.b .green}

- When algorithms learn from data, they create **models**

::: {.mt1 .fragment}
- This workshop is about creating [predictive models]{.b .green}

- Our goal will be to **use known/available information**...

- To **predict unknown/unavailable information**...

- Ideally in a way that generalized to **novel/unseen data**
:::

::: footer
*Note.* ML researchers also create models for other purposes (e.g., data generation).
:::

## Roles in Predictive Modeling {.f2}

[Labels / Outcomes]{.b .blue}

- Labels are [variables we want to predict]{.b .blue} the unknown values of

- Labels tend to be **expensive** or **difficult to measure** in new data

- Labels may also describe **subjective** experiences or **future** events

- *e.g., diagnoses, treatment response, substance use, suicide attempts*

::: {.mt1 .fragment}
[Features / Predictors]{.b .green}

- Features are [variables we use to predict]{.b .green} the unknown label values

- Features tend to be relatively **cheap** and **easy to measure** in new data

- *e.g., demographics, self-reports, digital behavior, neurobiology*
:::

## Modes of Predictive Modeling {.f2}

[Classification]{.b .blue}

- When labels are [categorical]{.b .blue}, predicting them is called "classification"

- *e.g., Will this patient respond to treatment or not?*

- *e.g., Is this patient depressed, euthymic, or manic?*

::: {.mt1 .fragment}
[Regression]{.b .green}

- When labels are [continuous]{.b .green}, predicting them is called "regression"

- *e.g., How many days will the patient be hospitalized for?*

- *e.g., How much of the drug will be in the patient's blood?*
:::

::: footer
*Note.* Unsupervised ML (or "data mining") has no explicit labels.
:::

## A Delicate Balance {.f2}

- Any data we collect will contain a mixture of both [signal]{.b} and [noise]{.b}

  - [Informative patterns]{.b .blue} that generalize to new data are the [signal]{.b .blue}
  - [Distracting patterns]{.b .green} specific to the original data are the [noise]{.b .green}
  
::: {.fragment}
- We want to capture as much signal and as little noise as possible

  - We accomplish this by finding the optimal *model complexity*
:::

::: {.fragment}
- More complex models capture *more signal* but also *more noise*
  
  - **Overfitting:** The model captures unwanted noise (too complex)
  - **Underfitting:** The model misses important signal (too simple)
:::

## Model Complexity

```{r complexity, message=FALSE, echo=FALSE}
library(tidyverse)
library(patchwork)

set.seed(2021)
signal <- function(x) {sin(2*pi*x)}
x_linspace <- seq(0, 1, by = 0.02)
x_data <- runif(length(x_linspace), 0, 1)
y_true <- signal(x_linspace)
y_data <- signal(x_data) + rnorm(length(x_data), 0, 0.25)
dat <- tibble(
  x_linspace,
  x_data,
  y_true,
  y_data
)

p1 <- 
  ggplot(dat) + 
  geom_point(aes(x = x_data, y = y_data), shape = 21, size = 3, color = "black", stroke = 1.5) + 
  stat_smooth(
    aes(x = x_data, y = y_data), 
    method = "lm", 
    se = FALSE,
    formula = y ~ x, 
    color = "#7570b3",
    size = 2
  ) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "Underfitting (d=1)", x = "feature", y = "label") +
  theme_gray(base_size = 18) +
  theme(
    panel.grid.minor = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

p2 <- 
  ggplot(dat) + 
  geom_point(aes(x = x_data, y = y_data), shape = 21, size = 3, stroke = 1.5, color = "black") + 
  stat_smooth(
    aes(x = x_data, y = y_data), 
    method = "lm",
    se = FALSE,
    formula = y ~ poly(x, 3), 
    color = "#1b9e77",
    size = 2
  ) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "Good Fit (d=3)", x = "feature", y = NULL) +
  theme_gray(base_size = 18) +
  theme(
    panel.grid.minor = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

p3 <- 
  ggplot(dat) + 
  geom_point(aes(x = x_data, y = y_data), shape = 21, size = 3, stroke = 1.5, color = "black") + 
  stat_smooth(
    aes(x = x_data, y = y_data), 
    method = "lm",
    se = FALSE,
    formula = y ~ poly(x, 16), 
    color = "#d95f02",
    size = 2
  ) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "Overfitting (d=16)", x = "feature", y = NULL) +
  theme_gray(base_size = 18) +
  theme(
    panel.grid.minor = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

(p1 | p2 | p3)
```

::: footer
*Note.* Complexity here is controlled by the highest polynomial degree $(d)$.
:::

## Overfitting Memes

![](./img/overfitting_lay.png)

## A Super Metaphor {.f2}

- ML's [superpower]{.b .blue} is its ability to [learn complex patterns]{.b .blue}

::: {.fragment}
- However, its [kryptonite]{.b .green} (i.e., weakness) is its [propensity to overfit]{.b .green}
:::

::: {.fragment}
- Thus, we must be careful to **detect** and **counteract** overfitting
:::

::: {.fragment}
- Our primary tool for these goals is **data splitting**

  - We learn feature-label relationships and select the optimal parameter values using one part (the [training set]{.b .blue})
  
  - We evaluate the model's ability to generalize to new data in another part that it has never seen before (the [testing set]{.b .green})
:::

::: footer
*Note.* Cross-validation is a sophisticated form of data splitting.
:::

## A Simple Training Set

```{r overex1, echo=FALSE}
set.seed(2021)
training <- data.frame(
  x = c(1:5) * 20,
  y = c(21, 20, 28, 24, 30) * 3
)
testing <- data.frame(
  x = c(1.5, 2, 3, 4.5) * 20,
  y = c(22, 24, 22, 27) * 3
)
alldata <- bind_rows(training, testing, .id = "Dataset") %>%
  mutate(Dataset = factor(Dataset, labels = c("Training", "Testing")))

config <-   
  theme_grey(base_size = 18) +
  theme(panel.grid.minor = element_blank())

ggplot(training, aes(x, y)) + 
  geom_point(size = 6, color = "#d95f02") +
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = "Test Score") +
  theme_grey(base_size = 24) +
  theme(panel.grid.minor = element_blank())
```

## Two Competing Models {.f2}

::: columns

::: column
```{r overex2, echo=FALSE}
#| fig.width: 4
#| fig.height: 4
#| out.width: 100%

ggplot(training, aes(x, y)) + 
  geom_point(size = 6, color = "#d95f02") +
  geom_smooth(method = "lm", size = 1.5, color = "black",
              se = FALSE, formula = y~x, linetype = "dashed") +
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = "Test Score",
       title = "Model A (d=1)") +
  config
```

:::

::: column
```{r overex3, echo=FALSE}
#| fig.width: 4
#| fig.height: 4
#| out.width: 100%

ggplot(training, aes(x, y)) + 
  geom_point(size = 6, color = "#d95f02") +
  ggalt::geom_xspline(color = "black", size = 1.5, spline_shape = -1, linetype = "dashed") + 
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = NULL,
       title = "Model B (d=4)") +
  config
```

:::
:::

## Bias (Error in Training Set) {.f2}

::: columns

::: column
```{r overex2b, echo=FALSE}
#| fig.width: 4
#| fig.height: 4
#| out.width: 100%

ggplot(training, aes(x, y)) + 
  geom_point(size = 6, color = "#d95f02") +
  geom_smooth(method = "lm", size = 1.5, color = "black",
              se = FALSE, formula = y~x, linetype = "dashed") +
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = "Test Score",
       title = "Model A (d=1)") +
  config
```
Training Error A = 31.2 ðŸ˜Ÿ
:::

::: column
```{r overex3b, echo=FALSE}
#| fig.width: 4
#| fig.height: 4
#| out.width: 100%

ggplot(training, aes(x, y)) + 
  geom_point(size = 6, color = "#d95f02") +
  ggalt::geom_xspline(color = "black", size = 1.5, spline_shape = -1, linetype = "dashed") + 
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = NULL,
       title = "Model B (d=4)") +
  config
```
Training Error B = 0.0 ðŸ¤©
:::
:::

## A Simple Testing Set

```{r overex4, echo=FALSE}
ggplot(mapping = aes(x, y)) +
  geom_point(
    data = alldata, 
    aes(shape = Dataset, color = Dataset), 
    size = 6
  ) +
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = "Test Score") +
  theme_grey(base_size = 24) +
  theme(panel.grid.minor = element_blank(),
        legend.spacing.y = unit(0.5, 'cm')) +
  guides(
    color = guide_legend(byrow = TRUE), 
    shape = guide_legend(byrow = TRUE)
  )
```

## A Rematch in New Data {.f2}

::: columns
::: column
```{r overex5, echo=FALSE}
#| fig.width: 4
#| fig.height: 4
#| out.width: 100%

ggplot(mapping = aes(x, y)) +
  geom_point(
    data = testing, 
    shape = "triangle",
    color = "#00BFC4",
    size = 6
  ) +
  geom_smooth(
    data = training,
    method = "lm",
    se = FALSE,
    color = "black",
    size = 1.5,
    linetype = "dashed"
  ) + 
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = "Test Score",
       title = "Model A (d=1)") +
  config
```
:::

::: column
```{r overex6, warning=FALSE, echo=FALSE}
#| fig.width: 4
#| fig.height: 4
#| out.width: 100%

ggplot(mapping = aes(x, y)) +
  geom_point(
    data = testing, 
    shape = "triangle",
    color = "#00BFC4",
    size = 6
  ) +
  ggalt::geom_xspline(
    data = training,
    spline_shape = -1,
    color = "black",
    size = 1.5,
    linetype = "dashed",
    aes(outfit = fity <<-..y..)
  ) + 
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = "Test Score",
       title = "Model B (d=4)") +
  config
```
:::
:::

## Variance (Error in Testing Set) {.f2}

::: columns
::: column
```{r overex5b, echo=FALSE}
#| fig.width: 4
#| fig.height: 4
#| out.width: 100%

ggplot(mapping = aes(x, y)) +
  geom_point(
    data = testing, 
    shape = "triangle",
    color = "#00BFC4",
    size = 6
  ) +
  geom_smooth(
    data = training,
    method = "lm",
    se = FALSE,
    color = "black",
    size = 1.5,
    linetype = "dashed"
  ) + 
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = "Test Score",
       title = "Model A (d=1)") +
  config
```
Testing Error A = 17.4 ðŸ™‚
:::

::: column
```{r overex6b, warning=FALSE, echo=FALSE}
#| fig.width: 4
#| fig.height: 4
#| out.width: 100%

ggplot(mapping = aes(x, y)) +
  geom_point(
    data = testing, 
    shape = "triangle",
    color = "#00BFC4",
    size = 6
  ) +
  ggalt::geom_xspline(
    data = training,
    spline_shape = -1,
    color = "black",
    size = 1.5,
    linetype = "dashed",
    aes(outfit = fity <<-..y..)
  ) + 
  coord_cartesian(ylim = c(50, 100), xlim = c(20, 100)) +
  labs(x = "Study Time", y = "Test Score",
       title = "Model B (d=4)") +
  config
```
Testing Error B = 41.6 ðŸ˜¨
:::
:::

## Conclusions from Example {.f2}

- In ML, model **"bias"** is prediction error in the original data

- In ML, model **"variance"** is prediction error in new/unseen data

::: {.fragment}
- An ideal model would have low bias and low variance

- However, there is often an inherent **bias-variance tradeoff**

  - We may counterintuitively prefer a model with more bias...
  - Because accuracy *in new data* is the most important thing!
:::


::: {.fragment}
- We want to the model that is as simple as possible but no simpler

- We will thus "tune" our models to try to achieve this balance

  - We will search for parameter values with the least variance
:::

## Tuning Complexity for Variance

![](./img/overfitting.png)


## Machine Learning Steps {.f2}

- [Data Preparation]{.b .blue}: Importing, Tidying, Splitting
  
::: {.fragment}
- [Model Development]{.b .blue}: Feature Engineering, Model Specification
:::

::: {.fragment}
- [Model Tuning]{.b .blue}: Resampling, Optimization, Selection
:::

::: {.fragment}
- [Model Evaluation]{.b .blue}: Performance, (Comparison), (Utility)
:::

::: {.fragment}
- [Model Use]{.b .blue}: Explanation, (Deployment), (Updating)
:::

<!-- Classification Example -->

# Classification Example
Predict cocaine use from demographics and personality

## 1. Import and tidy data

We load our data from a local text (i.e., CSV) file

We coerce all categorical variables to explicit factors

We retain only features/predictors and our label/outcome

```{r, eval=FALSE, echo=TRUE}
# Load the tidyverse package for data reading and tidying
library(tidyverse)

drugs <- 
  # Read in data from CSV file
  read_csv("./data/drug_consumption.csv", show_col_types = FALSE) |> 
  # Transform categorical variables into factors
  mutate(
    across(c(Age:Ethnicity, Caff:Nicotine), factor),
    Coke = factor(Coke, levels = c(1, 0))  # Set positive class first
  ) |> 
  # Drop unneeded variables
  select(Age:Nicotine, Coke)
```

::: footer
*Note.* Read ["R for Data Science"](https://r4ds.had.co.nz/) to learn about efficient data tidying in R.
:::

## 2. Split data for training and testing

We use 80% of the data for "training" and 20% for "testing"

We stratify both sets by our outcome variable (cocaine use)

```{r, eval=FALSE, echo=TRUE}
# Load the tidymodels package for machine learning
library(tidymodels)

# Set random number generation seed for reproducibility
set.seed(2022)

# Create initial split for holdout validation
coke_split <- initial_split(drugs, prop = 0.8, strata = Coke)

# Extract and save the training set (for development and tuning)
coke_train <- training(coke_split)

# Extract and save the testing set (for evaluation only)
coke_test <- testing(coke_split)
```

::: footer
*Note.* Stratification ensures a similar distribution across all splits.
:::

## 3. Prepare feature engineering recipe

We assign a role ("outcome" or "predictor") to each variable

We engineer (e.g., transform and filter) the features/predictors

```{r, eval=FALSE, echo=TRUE}
coke_recipe <-
  # Initialize the recipe from the training set
  recipe(coke_train) |> 
  # Assign the outcome role to the label variable
  update_role(Coke, new_role = "outcome") |> 
  # Assign the predictor role to the feature variables
  update_role(Age:Nicotine, new_role = "predictor") |> 
  # Dummy code all nominal (i.e., factor) predictors
  step_dummy(all_nominal_predictors()) |> 
  # Remove predictors with near zero variance
  step_nzv(all_predictors()) |> 
  # Remove predictors that are highly inter-correlated
  step_corr(all_predictors()) |> 
  # Remove predictors that are linear combinations
  step_lincomb(all_predictors())
```

::: footer
*Note.* Many other feature engineering steps are available from [{recipes}](https://recipes.tidymodels.org/).
:::

## 4. Set up model and parameters

We specify a Random Forest classifier using {ranger}

We specify that all three model parameters should be tuned

We set the "importance" metric for later model explanations

```{r, eval=FALSE, echo=TRUE}
# Optional: view documentation for the random forest algorithm
?rand_forest

coke_model <-
  # Select the algorithm and which parameters to tune
  rand_forest(mtry = tune(), trees = tune(), min_n = tune()) |> 
  # Select the prediction mode (classification or regression)
  set_mode("classification") |> 
  # Select the engine to use and, optionally, the importance metric
  set_engine("ranger", importance = "impurity")
```

::: footer
*Note.* Many other algorithms and engines are available from [{parsnip}](https://parsnip.tidymodels.org/).
:::

## 5. Prepare workflow

We combine our recipe and model into a "workflow" object

This helps R to repeat everything correctly during tuning

```{r, eval=FALSE, echo=TRUE}
# Combine recipe and model specification into a workflow
coke_wflow <-
  workflow() |> 
  add_recipe(coke_recipe) |> 
  add_model(coke_model)
```

::: footer
*Note.* We could also combine multiple workflows into [{workflowsets}](https://workflowsets.tidymodels.org).
:::

## 6. Tune parameter values

We split the training set into folds for cross-validation

We try lots of different values and record their performance

```{r, eval=FALSE, echo=TRUE}
# Create 10-fold CV within training set for tuning
coke_folds <- vfold_cv(coke_train, v = 10, strata = Coke)

# Pick reasonable boundaries for the tuning parameters
coke_param <-
  extract_parameter_set_dials(coke_model) |> 
  finalize(coke_folds)

# Create and search over a grid of parameter values within boundaries
coke_tune <-
  coke_wflow |> 
  tune_grid(
    resamples = coke_folds,
    param_info = coke_param,
    grid = 20 # how many combinations to try? (more is better but slower)
  )
```

::: footer
*Note.* Other tuning procedures are available from [{tune}](https://tune.tidymodels.org) and  [{finetune}](https://finetune.tidymodels.org).
:::

## 7. Finalize the workflow

We select our final parameter values from the tuning results

We refit to the entire training set and apply it to the testing set

```{r, eval=FALSE, echo=TRUE}
# Select the best parameters values
coke_param_final <- select_best(coke_tune, metric = "accuracy")

# Finalize the workflow with the best parameter values
coke_wflow_final <-
  coke_wflow |> 
  finalize_workflow(coke_param_final)

# Fit the finalized workflow to training set and apply it to testing set
coke_final <-
  coke_wflow_final |> 
  last_fit(coke_split)
```

::: footer
*Note.* More complicated selection procedures are available from [{tune}](https://tune.tidymodels.org).
:::

## 8. Evaluate model performance

We compare the test set predictions to their labels

We make a confusion matrix and ROC curve (for classification)

```{r, eval=FALSE, echo=TRUE}
# View the metrics (from the testing set)
collect_metrics(coke_final)

# Collect predictions (from the testing set)
coke_pred <- collect_predictions(coke_final)
coke_pred

# Calculate and plot confusion matrix
coke_cm <- conf_mat(coke_pred, truth = Coke, estimate = .pred_class)
coke_cm
autoplot(coke_cm, type = "heatmap")
summary(coke_cm)

# Calculate and plot ROC curve
coke_rc <- roc_curve(coke_pred, truth = Coke, .pred_1)
autoplot(coke_rc)
```

::: footer
*Note.* Many other performance tools are available from [{yardstick}](https://yardstick.tidymodels.org).
:::

## Bonus: Explore variable importance

Which variables were most important to prediction?

- Not all algorithms support variable importance measures

```{r, eval=FALSE, echo=TRUE}
# Extract the variable importance measures
coke_final |> 
  extract_fit_parsnip() |> 
  vi()

# Plot the variable importance measures
coke_final |> 
  extract_fit_parsnip() |> 
  vip()
```

::: footer
*Note.* Learn more advanced explainable AI techniques [here](https://ema.drwhy.ai/) or [here](https://christophm.github.io/interpretable-ml-book/).
:::

## Putting it all together

```{r, eval=FALSE, echo=TRUE}
library(tidyverse)
library(tidymodels)
library(vip)

# 1. Import and tidy data
drugs <- 
  read_csv("./data/drug_consumption.csv", show_col_types = FALSE) |> 
  mutate(
    across(c(Age:Ethnicity, Caff:Nicotine), factor),
    Coke = factor(Coke, levels = c(1, 0))
  ) |> 
  select(Age:Nicotine, Coke)

# 2. Split data for training and testing
set.seed(2022)
coke_split <- initial_split(drugs, prop = 0.8, strata = Coke)
coke_train <- training(coke_split)
coke_test <- testing(coke_split)

# 3. Prepare preprocessing recipe
coke_recipe <-
  recipe(coke_train) |> 
  update_role(Coke, new_role = "outcome") |> 
  update_role(Age:Nicotine, new_role = "predictor") |> 
  step_dummy(all_nominal_predictors()) |> 
  step_nzv(all_predictors()) |> 
  step_corr(all_predictors()) |> 
  step_lincomb(all_predictors())

# 4. Set up model and parameters
coke_model <-
  rand_forest(mtry = tune(), trees = tune(), min_n = tune()) |> 
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")

# 5. Prepare workflow
coke_wflow <-
  workflow() |> 
  add_recipe(coke_recipe) |> 
  add_model(coke_model)

# 6. Tune parameters using grid search
coke_folds <- vfold_cv(coke_train, v = 10, strata = Coke)
coke_param <-
  extract_parameter_set_dials(coke_model) |> 
  finalize(coke_folds)
coke_tune <-
  coke_wflow |> 
  tune_grid(
    resamples = coke_folds,
    param_info = coke_param,
    grid = 20
  )

# 7. Finalize the workflow
coke_param_final <- select_best(coke_tune, metric = "accuracy")
coke_wflow_final <-
  coke_wflow |> 
  finalize_workflow(coke_param_final)
coke_final <-
  coke_wflow_final |> 
  last_fit(coke_split)

# 8. Evaluate model performance
collect_metrics(coke_final)
coke_pred <- collect_predictions(coke_final)
coke_cm <- conf_mat(coke_pred, truth = Coke, estimate = .pred_class)
autoplot(coke_cm, type = "heatmap")
summary(coke_cm)
coke_rc <- roc_curve(coke_pred, truth = Coke, .pred_0)
autoplot(coke_rc)

# Bonus: Explore variable importance
coke_final |> 
  extract_fit_parsnip() |> 
  vi()
coke_final |> 
  extract_fit_parsnip() |> 
  vip()
```

<!-- Regression Example -->

# Regression Example

Predict median housing value from block information

## Necessary Changes

To adapt this process to regression, we only need to:

- Assign a continuous variable to the "outcome" role

- Select an algorithm that supports regression

  (Note that some support both, e.g., random forest)
  
- Set the prediction mode to "regression"

- Use regression performance metrics (e.g., RMSE or $R^2$)

- Skip the confusion matrix and ROC curve

  (We can instead plot predictions against trusted labels)

## Putting it all together... again

```{r, eval=FALSE, echo=TRUE}
library(tidyverse)
library(tidymodels)
library(vip)

# 1. Import and tidy data
calihouse <- read_csv("./data/calihouse_2000.csv", show_col_types = FALSE)

# 2. Split data for training and testing
set.seed(2022)
ch_split <- initial_split(calihouse, prop = 0.8, strata = house_mdn_value)
ch_train <- training(ch_split)
ch_test <- testing(ch_split)

# 3. Prepare preprocessing recipe
ch_recipe <-
  recipe(ch_train) |> 
  update_role(house_mdn_value, new_role = "outcome") |>
  update_role(house_mdn_age:dist_sj, new_role = "predictor") |> 
  step_nzv(all_predictors()) |> 
  step_corr(all_predictors()) |> 
  step_lincomb(all_predictors()) |> 
  step_normalize(all_numeric_predictors())

# 4. Set up model and parameters
ch_model <-
  rand_forest(mtry = tune(), trees = tune(), min_n = tune()) |> 
  set_mode("regression") |>
  set_engine("ranger", importance = "impurity")

# 5. Prepare workflow
ch_wflow <-
  workflow() |> 
  add_recipe(ch_recipe) |> 
  add_model(ch_model)

# 6. Tune parameters using grid search
ch_folds <- vfold_cv(ch_train, v = 10, strata = house_mdn_value)
ch_param <-
  ch_model |> 
  extract_parameter_set_dials() |> 
  finalize(ch_folds)
ch_tune <-
  ch_wflow |> 
  tune_grid(
    resamples = ch_folds,
    param_info = ch_param,
    grid = 20
  )

# 7. Finalize the workflow
ch_param_final <- select_best(ch_tune, metric = "rmse")
ch_wflow_final <-
  ch_wflow |> 
  finalize_workflow(ch_param_final)
ch_final <-
  ch_wflow_final |> 
  last_fit(ch_split)

# 8. Evaluate model performance
collect_metrics(ch_final)
ch_pred <- collect_predictions(ch_final)
ggplot(ch_pred, aes(x = house_mdn_value, y = .pred)) +
  geom_point(alpha = 1/5) +
  geom_abline() +
  coord_obs_pred()

# Bonus: Explore variable importance
ch_final |> 
  extract_fit_parsnip() |> 
  vi()
ch_final |> 
  extract_fit_parsnip() |> 
  vip()
```

<!-- Advanced Previews -->

# Advanced Previews

## Data Splitting and Preprocessing {.f30}

::: columns

::: column
- Clustered and longitudinal data
  
  - `rsample::group_vfold_cv()`
  - `rsample::rolling_origin()`

- Nested cross-validation

  - `rsample::nested_cv()`
:::

::: column
::: {.fragment}
- Dimensionality Reduction

  - `recipes::step_pca()`
  - `recipes::step_nnmf()`

- Imputing missing predictors

  - `recipes::step_impute_linear()`
  - `recipes::step_impute_knn()`

:::
:::
:::

## Development and Tuning {.f30}

- Alternative algorithms

  - `parsnip::linear_reg(penalty, mixture)`
  - `parsnip::logistic_reg(penalty, mixture)`
  - `parsnip::svm_rbf(cost)`

::: {.fragment}
- Alternative tuning procedures

  - `finetune::tune_sim_anneal()`
  - `finetune::tune_race_anova()`
:::

::: {.fragment}
- Combining models into ensembles

  - `stacks::stacks()`
  - `stacks::blend_predictions()`
:::

## Model Evaluation {.f30}

- Workflow sets and parallelization

  - `workflowsets::workflow_set()`
  - `tune::control_grid()`

::: {.fragment}
- Advanced/alternative metrics

  - `yardstick::mcc()` or `yardstick::mn_log_loss()`
  - `yardstick::ccc()` or `yardstick::huber_loss()`
:::

::: {.fragment}
- Statistical model comparison

  - `tidyposterior::perf_mod()`
  - `tidyposterior::contrast_models()`
:::

::: footer
*Note.* The {yardstick} team is currently adding calibration metrics.
:::

## Explanation and Deployment {.f30}

- Advanced model explanations

  - `DALEXtra::explain_tidymodels()`
  - `DALEXtra::model_parts()`
  - `DALEXtra::predict_parts()`
  - `DALEXtra::model_profile()`

::: {.fragment}
- Bundling and deploying models

  - `bundle::bundle()`
  - `vetiver::vetiver_model()`
:::


<!-- Question and Answer -->

# Question and Answer

## Resources {.smaller}

- **Online Textbook:**

  Tidy Modeling with R (Kuhn & Silge)
  
  <https://www.tmwr.org>

::: {.mt1}
- **Package Reference Website:**

  The tidymodels framework (RStudio/Posit)
  
  <https://www.tidymodels.org>
:::

::: {.mt1}
- **Four-day summer course:**

  Applied Machine Learning in R (Girard & Wang)

  <https://www.pittmethods.com/applied-ml>
:::

<!-- Practice Assignments -->

# Practice Assignments

## Practice Assignments {.f2}

1. Refit the model from Walkthrough #1 using regularized logistic regression classification and the GLMNET engine:

```{r, eval=FALSE, echo=TRUE}
logistic_reg(penalty = "tune", mixture = "tune") |> 
set_mode("classification") |> 
set_engine("glmnet")
```

::: {.mt2}
2. Adjust the model from Walkthrough #1 to predict Methamphetamine use (`Meth`) rather than Cocaine use (`Coke`)
:::

::: {.mt2}
3. Adjust the model from Walkthrough #2 to search over a larger grid of 125 parameter values (i.e., 5 unique values for each parameter or $5^3$)
:::

::: {.mt2}
4. Explore the additional datasets on the [workshop website](https://jmgirard.github.io/tips2022ml/datasets) and adapt your code to build a predictive model for one of these datasets.
:::


